{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-08T23:05:48.542764Z",
     "start_time": "2024-09-08T23:00:21.522767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cc.de.300.vec.gz...\n",
      "cc.de.300.vec.gz downloaded.\n",
      "Loading the Word2Vec model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 31\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Load the pre-trained fastText German model (binary=False because it's in text format)\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading the Word2Vec model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 31\u001B[0m word2vec_model \u001B[38;5;241m=\u001B[39m \u001B[43mKeyedVectors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel loaded successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Function to calculate average Word2Vec vector for a sentence\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Thesis\\Bachelor\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001B[0m, in \u001B[0;36mKeyedVectors.load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[0;32m   1672\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   1673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_word2vec_format\u001B[39m(\n\u001B[0;32m   1674\u001B[0m         \u001B[38;5;28mcls\u001B[39m, fname, fvocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m, unicode_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1675\u001B[0m         limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, datatype\u001B[38;5;241m=\u001B[39mREAL, no_header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1676\u001B[0m     ):\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001B[39;00m\n\u001B[0;32m   1678\u001B[0m \n\u001B[0;32m   1679\u001B[0m \u001B[38;5;124;03m    Warnings\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1717\u001B[0m \n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1720\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1721\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Thesis\\Bachelor\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001B[0m, in \u001B[0;36m_load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[0;32m   2065\u001B[0m         _word2vec_read_binary(\n\u001B[0;32m   2066\u001B[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001B[0;32m   2067\u001B[0m         )\n\u001B[0;32m   2068\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2069\u001B[0m         \u001B[43m_word2vec_read_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcounts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2070\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kv\u001B[38;5;241m.\u001B[39mvectors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(kv):\n\u001B[0;32m   2071\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   2072\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mduplicate words detected, shrinking matrix size from \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2073\u001B[0m         kv\u001B[38;5;241m.\u001B[39mvectors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlen\u001B[39m(kv),\n\u001B[0;32m   2074\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Thesis\\Bachelor\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1971\u001B[0m, in \u001B[0;36m_word2vec_read_text\u001B[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001B[0m\n\u001B[0;32m   1969\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_word2vec_read_text\u001B[39m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):\n\u001B[0;32m   1970\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line_no \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(vocab_size):\n\u001B[1;32m-> 1971\u001B[0m         line \u001B[38;5;241m=\u001B[39m \u001B[43mfin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1972\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m line \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m   1973\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\gzip.py:427\u001B[0m, in \u001B[0;36mGzipFile.readline\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadline\u001B[39m(\u001B[38;5;28mself\u001B[39m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    426\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_not_closed()\n\u001B[1;32m--> 427\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\_compression.py:68\u001B[0m, in \u001B[0;36mDecompressReader.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b) \u001B[38;5;28;01mas\u001B[39;00m view, view\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m byte_view:\n\u001B[1;32m---> 68\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbyte_view\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m         byte_view[:\u001B[38;5;28mlen\u001B[39m(data)] \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\gzip.py:537\u001B[0m, in \u001B[0;36m_GzipReader.read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    535\u001B[0m     uncompress \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39mdecompress(buf, size)\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 537\u001B[0m     uncompress \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decompressor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecompress\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39munused_data \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;66;03m# Prepend the already read bytes to the fileobj so they can\u001B[39;00m\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;66;03m# be seen by _read_eof() and _read_gzip_header()\u001B[39;00m\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mprepend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39munused_data)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# URL for the pre-trained fastText German model\n",
    "fasttext_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz'\n",
    "model_filename = 'cc.de.300.vec.gz'\n",
    "\n",
    "# Function to download the model if it's not already present\n",
    "def download_model(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f'Downloading {filename}...')\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f'{filename} downloaded.')\n",
    "    else:\n",
    "        print(f'{filename} already exists.')\n",
    "\n",
    "# Download the fastText German model if it doesn't already exist\n",
    "download_model(fasttext_url, model_filename)\n",
    "\n",
    "# Load the pre-trained fastText German model (binary=False because it's in text format)\n",
    "print(\"Loading the Word2Vec model...\")\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_filename, binary=False)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Function to calculate average Word2Vec vector for a sentence\n",
    "def sentence_vector(sentence, model, vector_size):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found in the model\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Define the Word2Vec similarity function\n",
    "def word2vec_similarity(text1, text2, model, vector_size):\n",
    "    vec1 = sentence_vector(text1, model, vector_size)\n",
    "    vec2 = sentence_vector(text2, model, vector_size)\n",
    "    \n",
    "    # Check if either vector is all zeros (i.e., no valid words)\n",
    "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "        return 0  # Assign 0 similarity if one of the sentences is empty or has no valid words\n",
    "    \n",
    "    return cosine_similarity([vec1], [vec2])[0, 1]\n",
    "\n",
    "# Load the CSV file with test sentences\n",
    "df = pd.read_csv('testsätze.csv')\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row to compare the sentences\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the sentences from each column\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # List of comparisons: Satz 1 with Satz 2, Satz 1 with Satz 3, Satz 2 with Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Compare each sentence pair and calculate the Word2Vec cosine similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = word2vec_similarity(text1, text2, word2vec_model, vector_size=300)  # fastText uses 300 dimensions\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"Word2Vec Ähnlichkeit (Cosine)\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results in a formatted DataFrame\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc.de.300.vec.gz already exists.\n",
      "Loading the fastText Word2Vec model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 31\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Load the German fastText Word2Vec model in Word2Vec format (binary=False)\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading the fastText Word2Vec model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 31\u001B[0m word2vec_model \u001B[38;5;241m=\u001B[39m \u001B[43mKeyedVectors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel loaded successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Function to calculate average Word2Vec vector for a sentence\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Thesis\\Bachelor\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001B[0m, in \u001B[0;36mKeyedVectors.load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[0;32m   1672\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   1673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_word2vec_format\u001B[39m(\n\u001B[0;32m   1674\u001B[0m         \u001B[38;5;28mcls\u001B[39m, fname, fvocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m, unicode_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1675\u001B[0m         limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, datatype\u001B[38;5;241m=\u001B[39mREAL, no_header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1676\u001B[0m     ):\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001B[39;00m\n\u001B[0;32m   1678\u001B[0m \n\u001B[0;32m   1679\u001B[0m \u001B[38;5;124;03m    Warnings\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1717\u001B[0m \n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1720\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1721\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Thesis\\Bachelor\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001B[0m, in \u001B[0;36m_load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[0;32m   2065\u001B[0m         _word2vec_read_binary(\n\u001B[0;32m   2066\u001B[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001B[0;32m   2067\u001B[0m         )\n\u001B[0;32m   2068\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2069\u001B[0m         \u001B[43m_word2vec_read_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcounts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2070\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kv\u001B[38;5;241m.\u001B[39mvectors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(kv):\n\u001B[0;32m   2071\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   2072\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mduplicate words detected, shrinking matrix size from \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2073\u001B[0m         kv\u001B[38;5;241m.\u001B[39mvectors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlen\u001B[39m(kv),\n\u001B[0;32m   2074\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Thesis\\Bachelor\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1971\u001B[0m, in \u001B[0;36m_word2vec_read_text\u001B[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001B[0m\n\u001B[0;32m   1969\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_word2vec_read_text\u001B[39m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):\n\u001B[0;32m   1970\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line_no \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(vocab_size):\n\u001B[1;32m-> 1971\u001B[0m         line \u001B[38;5;241m=\u001B[39m \u001B[43mfin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1972\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m line \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m   1973\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\gzip.py:427\u001B[0m, in \u001B[0;36mGzipFile.readline\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadline\u001B[39m(\u001B[38;5;28mself\u001B[39m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    426\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_not_closed()\n\u001B[1;32m--> 427\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\_compression.py:68\u001B[0m, in \u001B[0;36mDecompressReader.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b) \u001B[38;5;28;01mas\u001B[39;00m view, view\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m byte_view:\n\u001B[1;32m---> 68\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbyte_view\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m         byte_view[:\u001B[38;5;28mlen\u001B[39m(data)] \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\gzip.py:537\u001B[0m, in \u001B[0;36m_GzipReader.read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    535\u001B[0m     uncompress \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39mdecompress(buf, size)\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 537\u001B[0m     uncompress \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decompressor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecompress\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39munused_data \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;66;03m# Prepend the already read bytes to the fileobj so they can\u001B[39;00m\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;66;03m# be seen by _read_eof() and _read_gzip_header()\u001B[39;00m\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mprepend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39munused_data)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# URL for the fastText German Word2Vec model\n",
    "model_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz'\n",
    "model_filename = 'cc.de.300.vec.gz'\n",
    "\n",
    "# Function to download the model if it's not already present\n",
    "def download_model(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f'Downloading {filename}...')\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f'{filename} downloaded.')\n",
    "    else:\n",
    "        print(f'{filename} already exists.')\n",
    "\n",
    "# Download the fastText German Word2Vec model if it doesn't already exist\n",
    "download_model(model_url, model_filename)\n",
    "\n",
    "# Load the German fastText Word2Vec model in Word2Vec format (binary=False)\n",
    "print(\"Loading the fastText Word2Vec model...\")\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_filename, binary=False)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Function to calculate average Word2Vec vector for a sentence\n",
    "def sentence_vector(sentence, model, vector_size):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found in the model\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Define the Word2Vec similarity function\n",
    "def word2vec_similarity(text1, text2, model, vector_size):\n",
    "    vec1 = sentence_vector(text1, model, vector_size)\n",
    "    vec2 = sentence_vector(text2, model, vector_size)\n",
    "    \n",
    "    # Check if either vector is all zeros (i.e., no valid words)\n",
    "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "        return 0  # Assign 0 similarity if one of the sentences is empty or has no valid words\n",
    "    \n",
    "    return cosine_similarity([vec1], [vec2])[0, 1]\n",
    "\n",
    "# Load the CSV file with test sentences\n",
    "df = pd.read_csv('testsätze.csv')\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row to compare the sentences\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the sentences from each column\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # List of comparisons: Satz 1 with Satz 2, Satz 1 with Satz 3, Satz 2 with Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Compare each sentence pair and calculate the Word2Vec cosine similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = word2vec_similarity(text1, text2, word2vec_model, vector_size=300)  # fastText uses 300 dimensions\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"Word2Vec Ähnlichkeit (Cosine)\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results in a formatted DataFrame\n",
    "print(results_df.to_string(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T23:16:20.140337Z",
     "start_time": "2024-09-08T23:08:24.941898Z"
    }
   },
   "id": "53dddbdd195c186",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "12a10911c6639e44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
