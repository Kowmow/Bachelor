{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-09T12:35:44.864618Z",
     "start_time": "2024-09-09T12:32:15.234554Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# URL for the pre-trained fastText German model\n",
    "fasttext_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz'\n",
    "model_filename = 'cc.de.300.vec.gz'\n",
    "\n",
    "# Function to download the model if it's not already present\n",
    "def download_model(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f'Downloading {filename}...')\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f'{filename} downloaded.')\n",
    "    else:\n",
    "        print(f'{filename} already exists.')\n",
    "\n",
    "# Download the fastText German model if it doesn't already exist\n",
    "download_model(fasttext_url, model_filename)\n",
    "\n",
    "# Load the pre-trained fastText German model (binary=False because it's in text format)\n",
    "print(\"Loading the Word2Vec model...\")\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_filename, binary=False)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Function to calculate average Word2Vec vector for a sentence\n",
    "def sentence_vector(sentence, model, vector_size):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found in the model\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Define the Word2Vec similarity function\n",
    "def word2vec_similarity(text1, text2, model, vector_size):\n",
    "    vec1 = sentence_vector(text1, model, vector_size)\n",
    "    vec2 = sentence_vector(text2, model, vector_size)\n",
    "    \n",
    "    # Check if either vector is all zeros (i.e., no valid words)\n",
    "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "        return 0  # Assign 0 similarity if one of the sentences is empty or has no valid words\n",
    "    \n",
    "    # Access the correct element of the similarity matrix\n",
    "    return cosine_similarity([vec1], [vec2])[0, 0]\n",
    "\n",
    "# Load the CSV file with test sentences\n",
    "df = pd.read_csv('tests√§tze.csv')\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row to compare the sentences\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the sentences from each column\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # List of comparisons: Satz 1 with Satz 2, Satz 1 with Satz 3, Satz 2 with Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Compare each sentence pair and calculate the Word2Vec cosine similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = word2vec_similarity(text1, text2, word2vec_model, vector_size=300)  # fastText uses 300 dimensions\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"Word2Vec √Ñhnlichkeit (Cosine)\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results in a formatted DataFrame\n",
    "print(results_df.to_string(index=False))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc.de.300.vec.gz already exists.\n",
      "Loading the Word2Vec model...\n",
      "Model loaded successfully.\n",
      "        Vergleich  Word2Vec √Ñhnlichkeit (Cosine)  Berechnungszeit (ms)\n",
      "Satz 1 mit Satz 2                       0.989480                0.4458\n",
      "Satz 1 mit Satz 3                       0.920521                0.2510\n",
      "Satz 2 mit Satz 3                       0.918425                0.2223\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from angle_emb import AnglE  # Prompts importieren wir nicht, da es zu einem Fehler f√ºhrt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Funktion zum Initialisieren des AnglE-Modells\n",
    "def load_angle_model(base_model_name, pretrained_lora_path):\n",
    "    print(f\"Lade Modell {pretrained_lora_path} mit Basis-Modell {base_model_name}...\")\n",
    "    angle = AnglE.from_pretrained(base_model_name, pretrained_lora_path=pretrained_lora_path)\n",
    "    print(f\"Modell erfolgreich geladen.\")\n",
    "    return angle\n",
    "\n",
    "# Funktion zum Enkodieren von S√§tzen und Berechnung der √Ñhnlichkeit\n",
    "def angle_similarity(text1, text2, model):\n",
    "    # S√§tze enkodieren\n",
    "    vec1 = model.encode({'text': text1}, to_numpy=True)\n",
    "    vec2 = model.encode({'text': text2}, to_numpy=True)\n",
    "    \n",
    "    # Cosine Similarity berechnen\n",
    "    similarity = cosine_similarity([vec1], [vec2])[0, 0]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Das Modell von AnglE und den LoRA-Pfad laden\n",
    "base_model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "pretrained_lora_path = \"SeanLee97/angle-llama-7b-nli-v2\"\n",
    "model = load_angle_model(base_model_name, pretrained_lora_path)\n",
    "\n",
    "# Lade die CSV-Datei mit den Tests√§tzen\n",
    "df = pd.read_csv('tests√§tze.csv')\n",
    "\n",
    "# Ergebnisse vorbereiten\n",
    "results = []\n",
    "\n",
    "# Iteriere √ºber jede Zeile, um die S√§tze zu vergleichen\n",
    "for index, row in df.iterrows():\n",
    "    # Extrahiere die S√§tze aus jeder Spalte\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # Liste der Vergleiche: Satz 1 mit Satz 2, Satz 1 mit Satz 3, Satz 2 mit Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Vergleiche jedes Satzpaar und berechne die Cosine Similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = angle_similarity(text1, text2, model)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        # Ergebnisse speichern\n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"Cosine Similarity\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Erstelle ein DataFrame aus den Ergebnissen\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Gib die Ergebnisse in einem formatierten DataFrame aus\n",
    "print(results_df.to_string(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T13:07:25.450657Z",
     "start_time": "2024-09-09T13:07:23.249732Z"
    }
   },
   "id": "53dddbdd195c186",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AnglE:LLM detected, automatically set is_llm=True.If it is wrong, you can manually set `is_llm`.\n",
      "INFO:AnglE:üö® LLM detected, but pooling strategy is specified to cls.Please check whether it is correct. It is recommended to use `last` pooling strategy for LLM.\n",
      "INFO:AnglE:LLM detected, automatically set apply_lora=True.If it is wrong, you can manually set `apply_lora`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell SeanLee97/angle-llama-7b-nli-v2 mit Basis-Modell NousResearch/Llama-2-7b-hf...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "358db0e899874f479fe4ce9bcfcb0857"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AnglE:Load lora weight from SeanLee97/angle-llama-7b-nli-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 54\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     53\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m---> 54\u001B[0m similarity \u001B[38;5;241m=\u001B[39m \u001B[43mangle_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m     57\u001B[0m elapsed_time_ms \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mround\u001B[39m((end_time \u001B[38;5;241m-\u001B[39m start_time) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m, \u001B[38;5;241m9\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 17\u001B[0m, in \u001B[0;36mangle_similarity\u001B[1;34m(text1, text2, model)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mangle_similarity\u001B[39m(text1, text2, model):\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;66;03m# S√§tze enkodieren\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m     vec1 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext1\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_numpy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     vec2 \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mencode({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m: text2}, to_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# Cosine Similarity berechnen\u001B[39;00m\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\angle_emb\\angle.py:1532\u001B[0m, in \u001B[0;36mAnglE.encode\u001B[1;34m(self, inputs, max_length, end_with_eos, to_numpy, embedding_start, embedding_size, device, prompt, normalize_embedding)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     tok \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mpad(tok, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_attention_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     tok \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1533\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1534\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlongest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1535\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1536\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1537\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1538\u001B[0m tok\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m   1539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3055\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   3053\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[0;32m   3054\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[1;32m-> 3055\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3056\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3057\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3114\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[0;32m   3111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   3113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_valid_text_input(text):\n\u001B[1;32m-> 3114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3115\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3116\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3117\u001B[0m     )\n\u001B[0;32m   3119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_valid_text_input(text_pair):\n\u001B[0;32m   3120\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3121\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3122\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3123\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from angle_emb import AnglE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Funktion zum Initialisieren des AnglE-Modells\n",
    "def load_angle_model(base_model_name, pretrained_lora_path):\n",
    "    print(f\"Lade Modell {pretrained_lora_path} mit Basis-Modell {base_model_name}...\")\n",
    "    angle = AnglE.from_pretrained(base_model_name, pretrained_lora_path=pretrained_lora_path)\n",
    "    print(f\"Modell erfolgreich geladen.\")\n",
    "    return angle\n",
    "\n",
    "# Funktion zum Enkodieren von S√§tzen und Berechnung der √Ñhnlichkeit\n",
    "def angle_similarity(text1, text2, model):\n",
    "    # √úberpr√ºfen, ob die Eingaben Strings sind\n",
    "    if not isinstance(text1, str) or not isinstance(text2, str):\n",
    "        raise ValueError(\"Beide Texteingaben m√ºssen Strings sein.\")\n",
    "    \n",
    "    # S√§tze enkodieren\n",
    "    vec1 = model.encode(text1, to_numpy=True)  # Direkter String als Input\n",
    "    vec2 = model.encode(text2, to_numpy=True)  # Direkter String als Input\n",
    "    \n",
    "    # Debugging: Die Dimensionen der Vektoren ausgeben\n",
    "    print(f\"Shape of vec1: {vec1.shape}\")\n",
    "    print(f\"Shape of vec2: {vec2.shape}\")\n",
    "    \n",
    "    # √úberpr√ºfen, ob die Vektoren 1D oder mehrdimensional sind\n",
    "    if len(vec1.shape) > 2 or len(vec2.shape) > 2:\n",
    "        raise ValueError(f\"Found array with dim {vec1.shape}, but expected <= 2.\")\n",
    "    \n",
    "    # Wenn sie 1D sind, in 2D umformen\n",
    "    if len(vec1.shape) == 1:\n",
    "        vec1 = vec1.reshape(1, -1)\n",
    "    if len(vec2.shape) == 1:\n",
    "        vec2 = vec2.reshape(1, -1)\n",
    "    \n",
    "    # Cosine Similarity berechnen\n",
    "    similarity = cosine_similarity(vec1, vec2)[0, 0]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Das Modell von AnglE und den LoRA-Pfad laden\n",
    "base_model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "pretrained_lora_path = \"SeanLee97/angle-llama-7b-nli-v2\"\n",
    "model = load_angle_model(base_model_name, pretrained_lora_path)\n",
    "\n",
    "# Lade die CSV-Datei mit den Tests√§tzen\n",
    "df = pd.read_csv('tests√§tze.csv')\n",
    "\n",
    "# Ergebnisse vorbereiten\n",
    "results = []\n",
    "\n",
    "# Iteriere √ºber jede Zeile, um die S√§tze zu vergleichen\n",
    "for index, row in df.iterrows():\n",
    "    # Extrahiere die S√§tze aus jeder Spalte\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # Liste der Vergleiche: Satz 1 mit Satz 2, Satz 1 mit Satz 3, Satz 2 mit Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Vergleiche jedes Satzpaar und berechne die Cosine Similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = angle_similarity(text1, text2, model)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        # Ergebnisse speichern\n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"Cosine Similarity\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Erstelle ein DataFrame aus den Ergebnissen\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Gib die Ergebnisse in einem formatierten DataFrame aus\n",
    "print(results_df.to_string(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T14:49:18.422047Z",
     "start_time": "2024-09-09T14:49:14.612403Z"
    }
   },
   "id": "12a10911c6639e44",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AnglE:LLM detected, automatically set is_llm=True.If it is wrong, you can manually set `is_llm`.\n",
      "INFO:AnglE:üö® LLM detected, but pooling strategy is specified to cls.Please check whether it is correct. It is recommended to use `last` pooling strategy for LLM.\n",
      "INFO:AnglE:LLM detected, automatically set apply_lora=True.If it is wrong, you can manually set `apply_lora`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell SeanLee97/angle-llama-7b-nli-v2 mit Basis-Modell NousResearch/Llama-2-7b-hf...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbefffc211384b4b884578df7d588203"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "INFO:AnglE:Load lora weight from SeanLee97/angle-llama-7b-nli-v2\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'base_model.model.model.model.embed_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 47\u001B[0m\n\u001B[0;32m     45\u001B[0m base_model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNousResearch/Llama-2-7b-hf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     46\u001B[0m pretrained_lora_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSeanLee97/angle-llama-7b-nli-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 47\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_angle_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# Lade die CSV-Datei mit den Tests√§tzen\u001B[39;00m\n\u001B[0;32m     50\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtests√§tze.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[6], line 10\u001B[0m, in \u001B[0;36mload_angle_model\u001B[1;34m(base_model_name, pretrained_lora_path)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_angle_model\u001B[39m(base_model_name, pretrained_lora_path):\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLade Modell \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_lora_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m mit Basis-Modell \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbase_model_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m     angle \u001B[38;5;241m=\u001B[39m \u001B[43mAnglE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModell erfolgreich geladen.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m angle\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\angle_emb\\angle.py:1298\u001B[0m, in \u001B[0;36mAnglE.from_pretrained\u001B[1;34m(model_name_or_path, pretrained_model_path, pretrained_lora_path, is_llm, pooling_strategy, train_mode, **kwargs)\u001B[0m\n\u001B[0;32m   1267\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m   1268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_pretrained\u001B[39m(model_name_or_path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   1269\u001B[0m                     pretrained_model_path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1273\u001B[0m                     train_mode: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1274\u001B[0m                     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1276\u001B[0m \u001B[38;5;124;03m    Load AnglE from pretrained model.\u001B[39;00m\n\u001B[0;32m   1277\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1296\u001B[0m \u001B[38;5;124;03m            angle.encode(*args, **kwargs)\u001B[39;00m\n\u001B[0;32m   1297\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1298\u001B[0m     angle \u001B[38;5;241m=\u001B[39m \u001B[43mAnglE\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1299\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mis_llm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_llm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1300\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mpretrained_model_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_model_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1301\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1302\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mpooling_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpooling_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1303\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtrain_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1304\u001B[0m \u001B[43m                  \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1305\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m angle\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\angle_emb\\angle.py:1154\u001B[0m, in \u001B[0;36mAnglE.__init__\u001B[1;34m(self, model_name_or_path, tokenizer_name_or_path, max_length, model_kwargs, lora_config_kwargs, pooling_strategy, apply_lora, train_mode, load_kbit, is_llm, pretrained_model_path, pretrained_lora_path, torch_dtype, device, kbit_kwargs, tokenizer_padding_side, apply_billm, billm_model_class, **kwargs)\u001B[0m\n\u001B[0;32m   1152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pretrained_lora_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1153\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoad lora weight from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_lora_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1154\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mPeftModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_kbit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_mode\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m train_mode:\n\u001B[0;32m   1162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget_modules\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m lora_config \u001B[38;5;129;01mor\u001B[39;00m lora_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget_modules\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\peft_model.py:545\u001B[0m, in \u001B[0;36mPeftModel.from_pretrained\u001B[1;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001B[0m\n\u001B[0;32m    540\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    541\u001B[0m     model \u001B[38;5;241m=\u001B[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001B[38;5;241m.\u001B[39mtask_type](\n\u001B[0;32m    542\u001B[0m         model, config, adapter_name, autocast_adapter_dtype\u001B[38;5;241m=\u001B[39mautocast_adapter_dtype\n\u001B[0;32m    543\u001B[0m     )\n\u001B[1;32m--> 545\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_adapter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_trainable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\peft_model.py:1151\u001B[0m, in \u001B[0;36mPeftModel.load_adapter\u001B[1;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001B[0m\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(device_map, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1147\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m infer_auto_device_map(\n\u001B[0;32m   1148\u001B[0m         \u001B[38;5;28mself\u001B[39m, max_memory\u001B[38;5;241m=\u001B[39mmax_memory, no_split_module_classes\u001B[38;5;241m=\u001B[39mno_split_module_classes\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[1;32m-> 1151\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_offload\u001B[49m\u001B[43m(\u001B[49m\u001B[43moffload_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapters_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1152\u001B[0m dispatch_model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffload_index\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m offload_index\n\u001B[0;32m   1154\u001B[0m dispatch_model(\n\u001B[0;32m   1155\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1156\u001B[0m     device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[0;32m   1157\u001B[0m     offload_dir\u001B[38;5;241m=\u001B[39moffload_dir,\n\u001B[0;32m   1158\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdispatch_model_kwargs,\n\u001B[0;32m   1159\u001B[0m )\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\peft_model.py:1028\u001B[0m, in \u001B[0;36mPeftModel._update_offload\u001B[1;34m(self, offload_index, adapters_weights)\u001B[0m\n\u001B[0;32m   1026\u001B[0m suffix_pos \u001B[38;5;241m=\u001B[39m safe_key\u001B[38;5;241m.\u001B[39mrfind(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1027\u001B[0m extended_prefix \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m block_id \u001B[38;5;241m+\u001B[39m safe_key[:suffix_pos]\n\u001B[1;32m-> 1028\u001B[0m safe_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnamed_modules\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mextended_prefix\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(safe_module, BaseTunerLayer):\n\u001B[0;32m   1030\u001B[0m     final_key \u001B[38;5;241m=\u001B[39m extended_prefix \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.base_layer\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m safe_key[suffix_pos:]\n",
      "\u001B[1;31mKeyError\u001B[0m: 'base_model.model.model.model.embed_tokens'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:24:57.107103Z",
     "start_time": "2024-09-09T13:24:54.577490Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "84544b5d4366da9a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AnglE:LLM detected, automatically set is_llm=True.If it is wrong, you can manually set `is_llm`.\n",
      "INFO:AnglE:üö® LLM detected, but pooling strategy is specified to cls.Please check whether it is correct. It is recommended to use `last` pooling strategy for LLM.\n",
      "INFO:AnglE:LLM detected, automatically set apply_lora=True.If it is wrong, you can manually set `apply_lora`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell SeanLee97/angle-llama-7b-nli-v2 mit Basis-Modell NousResearch/Llama-2-7b-hf...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "084046ca550d48bfa15ddde68646843e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "INFO:AnglE:Load lora weight from SeanLee97/angle-llama-7b-nli-v2\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'base_model.model.model.model.embed_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m base_model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNousResearch/Llama-2-7b-hf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     36\u001B[0m pretrained_lora_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSeanLee97/angle-llama-7b-nli-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 37\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_angle_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Lade die CSV-Datei mit den Tests√§tzen\u001B[39;00m\n\u001B[0;32m     40\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtests√§tze.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[5], line 11\u001B[0m, in \u001B[0;36mload_angle_model\u001B[1;34m(base_model_name, pretrained_lora_path)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_angle_model\u001B[39m(base_model_name, pretrained_lora_path):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLade Modell \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_lora_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m mit Basis-Modell \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbase_model_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 11\u001B[0m     angle \u001B[38;5;241m=\u001B[39m \u001B[43mAnglE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModell erfolgreich geladen.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m angle\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\angle_emb\\angle.py:1298\u001B[0m, in \u001B[0;36mAnglE.from_pretrained\u001B[1;34m(model_name_or_path, pretrained_model_path, pretrained_lora_path, is_llm, pooling_strategy, train_mode, **kwargs)\u001B[0m\n\u001B[0;32m   1267\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m   1268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_pretrained\u001B[39m(model_name_or_path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   1269\u001B[0m                     pretrained_model_path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1273\u001B[0m                     train_mode: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1274\u001B[0m                     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1276\u001B[0m \u001B[38;5;124;03m    Load AnglE from pretrained model.\u001B[39;00m\n\u001B[0;32m   1277\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1296\u001B[0m \u001B[38;5;124;03m            angle.encode(*args, **kwargs)\u001B[39;00m\n\u001B[0;32m   1297\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1298\u001B[0m     angle \u001B[38;5;241m=\u001B[39m \u001B[43mAnglE\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1299\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mis_llm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_llm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1300\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mpretrained_model_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_model_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1301\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1302\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mpooling_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpooling_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1303\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtrain_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1304\u001B[0m \u001B[43m                  \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1305\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m angle\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\angle_emb\\angle.py:1154\u001B[0m, in \u001B[0;36mAnglE.__init__\u001B[1;34m(self, model_name_or_path, tokenizer_name_or_path, max_length, model_kwargs, lora_config_kwargs, pooling_strategy, apply_lora, train_mode, load_kbit, is_llm, pretrained_model_path, pretrained_lora_path, torch_dtype, device, kbit_kwargs, tokenizer_padding_side, apply_billm, billm_model_class, **kwargs)\u001B[0m\n\u001B[0;32m   1152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pretrained_lora_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1153\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoad lora weight from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_lora_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1154\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mPeftModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_lora_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_kbit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_mode\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m train_mode:\n\u001B[0;32m   1162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget_modules\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m lora_config \u001B[38;5;129;01mor\u001B[39;00m lora_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget_modules\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\peft_model.py:545\u001B[0m, in \u001B[0;36mPeftModel.from_pretrained\u001B[1;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001B[0m\n\u001B[0;32m    540\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    541\u001B[0m     model \u001B[38;5;241m=\u001B[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001B[38;5;241m.\u001B[39mtask_type](\n\u001B[0;32m    542\u001B[0m         model, config, adapter_name, autocast_adapter_dtype\u001B[38;5;241m=\u001B[39mautocast_adapter_dtype\n\u001B[0;32m    543\u001B[0m     )\n\u001B[1;32m--> 545\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_adapter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_trainable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\peft_model.py:1151\u001B[0m, in \u001B[0;36mPeftModel.load_adapter\u001B[1;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001B[0m\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(device_map, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1147\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m infer_auto_device_map(\n\u001B[0;32m   1148\u001B[0m         \u001B[38;5;28mself\u001B[39m, max_memory\u001B[38;5;241m=\u001B[39mmax_memory, no_split_module_classes\u001B[38;5;241m=\u001B[39mno_split_module_classes\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[1;32m-> 1151\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_offload\u001B[49m\u001B[43m(\u001B[49m\u001B[43moffload_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapters_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1152\u001B[0m dispatch_model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffload_index\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m offload_index\n\u001B[0;32m   1154\u001B[0m dispatch_model(\n\u001B[0;32m   1155\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1156\u001B[0m     device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[0;32m   1157\u001B[0m     offload_dir\u001B[38;5;241m=\u001B[39moffload_dir,\n\u001B[0;32m   1158\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdispatch_model_kwargs,\n\u001B[0;32m   1159\u001B[0m )\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\peft_model.py:1028\u001B[0m, in \u001B[0;36mPeftModel._update_offload\u001B[1;34m(self, offload_index, adapters_weights)\u001B[0m\n\u001B[0;32m   1026\u001B[0m suffix_pos \u001B[38;5;241m=\u001B[39m safe_key\u001B[38;5;241m.\u001B[39mrfind(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1027\u001B[0m extended_prefix \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m block_id \u001B[38;5;241m+\u001B[39m safe_key[:suffix_pos]\n\u001B[1;32m-> 1028\u001B[0m safe_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnamed_modules\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mextended_prefix\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(safe_module, BaseTunerLayer):\n\u001B[0;32m   1030\u001B[0m     final_key \u001B[38;5;241m=\u001B[39m extended_prefix \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.base_layer\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m safe_key[suffix_pos:]\n",
      "\u001B[1;31mKeyError\u001B[0m: 'base_model.model.model.model.embed_tokens'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:56:11.819211Z",
     "start_time": "2024-09-09T14:55:06.652801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Lade das SBERT-Modell (deutsches Modell)\n",
    "print(\"Lade das SBERT-Modell...\")\n",
    "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "print(\"Modell erfolgreich geladen.\")\n",
    "\n",
    "# Funktion zur Berechnung der SBERT-Satz-√Ñhnlichkeit\n",
    "def sbert_similarity(text1, text2, model):\n",
    "    # Berechnung der Satz-Embeddings mit SBERT\n",
    "    embeddings = model.encode([text1, text2])\n",
    "    \n",
    "    # Berechnung der Cosine Similarity zwischen den beiden Embeddings\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0, 0]\n",
    "\n",
    "# CSV-Datei mit Tests√§tzen laden\n",
    "df = pd.read_csv('tests√§tze.csv')\n",
    "\n",
    "# Ergebnisse vorbereiten\n",
    "results = []\n",
    "\n",
    "# Iteriere √ºber jede Zeile, um die S√§tze zu vergleichen\n",
    "for index, row in df.iterrows():\n",
    "    # Extrahiere die S√§tze aus jeder Spalte\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # Liste der Vergleiche: Satz 1 mit Satz 2, Satz 1 mit Satz 3, Satz 2 mit Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Vergleiche jedes Satzpaar und berechne die Cosine Similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = sbert_similarity(text1, text2, sbert_model)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"SBERT √Ñhnlichkeit (Cosine)\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Erstelle ein DataFrame aus den Ergebnissen\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ergebnisse in einem formatierten DataFrame ausgeben\n",
    "print(results_df.to_string(index=False))"
   ],
   "id": "4c010874a7826688",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade das SBERT-Modell...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18a4ed625d9643349d76fe522d667471"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Finn\\.cache\\huggingface\\hub\\models--sentence-transformers--distiluse-base-multilingual-cased-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e77f0f10d74846a49c239253fcfa5926"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4b08c6f514c4266ac23d623ffd7ee99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d467a20f8ed3497bb36e098a13baf934"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/556 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57d64d8e5f9543aa814296917265274e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4859bd104dce4e55aa8d84eee498dee1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c529114b0a00429a9e86d369effc687d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bdddd71425343f8810c3a3e231edb34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "505972ffedc74a85b8f6b85c4add7edf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ae419e2bedc4976bc97082eed6932c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b263f8b18a84ca9b46000154b84b3ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8556c12c0c4b401ea5214144e8223b64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30e0494d3ff8428283872e3878d4d032"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53e3ae448f91494dbe0922aee65cc7d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "847edca3e07d49209106f550d58bbe67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8da446caad14489a97ab18905422b5e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e8eb217308d47a0b7b7913bdac391bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Vergleich  SBERT √Ñhnlichkeit (Cosine)  Berechnungszeit (ms)\n",
      "Satz 1 mit Satz 2                    0.909018               58.5371\n",
      "Satz 1 mit Satz 3                    0.919996               23.5237\n",
      "Satz 2 mit Satz 3                    0.824267               24.9107\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:59:39.073668Z",
     "start_time": "2024-09-09T14:58:41.171380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Lade das SBERT-Modell (deutsches Modell)\n",
    "print(\"Lade das SBERT-Modell...\")\n",
    "sbert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "print(\"Modell erfolgreich geladen.\")\n",
    "\n",
    "# Funktion zur Berechnung der SBERT-Satz-√Ñhnlichkeit\n",
    "def sbert_similarity(text1, text2, model):\n",
    "    # Berechnung der Satz-Embeddings mit SBERT\n",
    "    embeddings = model.encode([text1, text2])\n",
    "    \n",
    "    # Berechnung der Cosine Similarity zwischen den beiden Embeddings\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0, 0]\n",
    "\n",
    "# CSV-Datei mit Tests√§tzen laden\n",
    "df = pd.read_csv('tests√§tze.csv')\n",
    "\n",
    "# Ergebnisse vorbereiten\n",
    "results = []\n",
    "\n",
    "# Iteriere √ºber jede Zeile, um die S√§tze zu vergleichen\n",
    "for index, row in df.iterrows():\n",
    "    # Extrahiere die S√§tze aus jeder Spalte\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # Liste der Vergleiche: Satz 1 mit Satz 2, Satz 1 mit Satz 3, Satz 2 mit Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Vergleiche jedes Satzpaar und berechne die Cosine Similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = sbert_similarity(text1, text2, sbert_model)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"SBERT √Ñhnlichkeit (Cosine)\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Erstelle ein DataFrame aus den Ergebnissen\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ergebnisse in einem formatierten DataFrame ausgeben\n",
    "print(results_df.to_string(index=False))"
   ],
   "id": "bc5760cfa0aafc25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade das SBERT-Modell...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cea617e963bd4216a9b54b4ae9d808e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Finn\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64c7890880894d01b0ab5e42e9196bb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "288d0bf1fab84e71b07383a9b900510f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86e9fba22acc4fe39e0c4f836e074725"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55592cce948147e092aac4dfb4f14599"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9a886df6cbe4f8a9f3ae703cfb6d8d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53630fea340b41709d95a8b4f4f00af0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b75ec325d3a34080907e25ff1e1dc892"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "390814d88ea341bfaca91dc97d1f440f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a13dc013edd549f3b9fb63926b5bc402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6110365c4bc84f38b365b0d21bf9e1c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a38e53f6bb54440814ee531512048ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e91778d05034300b55945f29ca48d49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de5f231509904cadb6f5eb3140e7c48e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Vergleich  SBERT √Ñhnlichkeit (Cosine)  Berechnungszeit (ms)\n",
      "Satz 1 mit Satz 2                    0.950575               89.8289\n",
      "Satz 1 mit Satz 3                    0.870597               49.3794\n",
      "Satz 2 mit Satz 3                    0.811649               47.4749\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T15:01:50.125591Z",
     "start_time": "2024-09-09T15:01:26.839002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "# Load the LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, pretrained_lora_path)\n",
    "\n",
    "print(\"Model with LoRA loaded successfully.\")"
   ],
   "id": "8eae9b942c8b5ae0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e49a01a319e4679a019a166d08a2b8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with LoRA loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T15:06:07.909042Z",
     "start_time": "2024-09-09T15:06:06.364146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Funktion zum Initialisieren des scaling_sentemb-Modells\n",
    "def load_scaling_sentemb_model():\n",
    "    print(\"Lade das scaling_sentemb Modell...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    print(\"Modell erfolgreich geladen.\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Funktion zum Enkodieren von S√§tzen und Berechnung der √Ñhnlichkeit\n",
    "def scaling_sentemb_similarity(text1, text2, tokenizer, model):\n",
    "    # Tokenisierung der S√§tze\n",
    "    inputs_1 = tokenizer(text1, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs_2 = tokenizer(text2, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Vektoren berechnen\n",
    "    with torch.no_grad():\n",
    "        vec1 = model(**inputs_1).last_hidden_state.mean(dim=1).numpy()\n",
    "        vec2 = model(**inputs_2).last_hidden_state.mean(dim=1).numpy()\n",
    "    \n",
    "    # Cosine Similarity berechnen\n",
    "    similarity = cosine_similarity([vec1], [vec2])[0, 0]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Das scaling_sentemb Modell laden\n",
    "tokenizer, model = load_scaling_sentemb_model()\n",
    "\n",
    "# Lade die CSV-Datei mit den Tests√§tzen\n",
    "df = pd.read_csv('tests√§tze.csv')\n",
    "\n",
    "# Ergebnisse vorbereiten\n",
    "results = []\n",
    "\n",
    "# Iteriere √ºber jede Zeile, um die S√§tze zu vergleichen\n",
    "for index, row in df.iterrows():\n",
    "    # Extrahiere die S√§tze aus jeder Spalte\n",
    "    satz1 = row.get('Satz 1', \"\")\n",
    "    satz2 = row.get('Satz 2', \"\")\n",
    "    satz3 = row.get('Satz 3', \"\")\n",
    "\n",
    "    # Liste der Vergleiche: Satz 1 mit Satz 2, Satz 1 mit Satz 3, Satz 2 mit Satz 3\n",
    "    comparisons = [(\"Satz 1 mit Satz 2\", satz1, satz2), \n",
    "                   (\"Satz 1 mit Satz 3\", satz1, satz3),\n",
    "                   (\"Satz 2 mit Satz 3\", satz2, satz3)]\n",
    "    \n",
    "    # Vergleiche jedes Satzpaar und berechne die Cosine Similarity\n",
    "    for comparison_label, text1, text2 in comparisons:\n",
    "        if not text1 or not text2:\n",
    "            continue\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        similarity = scaling_sentemb_similarity(text1, text2, tokenizer, model)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        elapsed_time_ms = round((end_time - start_time) * 1000, 9)\n",
    "        \n",
    "        # Ergebnisse speichern\n",
    "        results.append({\n",
    "            \"Vergleich\": comparison_label,\n",
    "            \"Cosine Similarity\": similarity,\n",
    "            \"Berechnungszeit (ms)\": elapsed_time_ms\n",
    "        })\n",
    "\n",
    "# Erstelle ein DataFrame aus den Ergebnissen\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Gib die Ergebnisse in einem formatierten DataFrame aus\n",
    "print(results_df.to_string(index=False))"
   ],
   "id": "1bff978365babb6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade das scaling_sentemb Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. check_pairwise_arrays expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 59\u001B[0m\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     58\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m---> 59\u001B[0m similarity \u001B[38;5;241m=\u001B[39m \u001B[43mscaling_sentemb_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m     62\u001B[0m elapsed_time_ms \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mround\u001B[39m((end_time \u001B[38;5;241m-\u001B[39m start_time) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m, \u001B[38;5;241m9\u001B[39m)\n",
      "Cell \u001B[1;32mIn[11], line 28\u001B[0m, in \u001B[0;36mscaling_sentemb_similarity\u001B[1;34m(text1, text2, tokenizer, model)\u001B[0m\n\u001B[0;32m     25\u001B[0m     vec2 \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs_2)\u001B[38;5;241m.\u001B[39mlast_hidden_state\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Cosine Similarity berechnen\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m similarity \u001B[38;5;241m=\u001B[39m \u001B[43mcosine_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mvec1\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mvec2\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m similarity\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1679\u001B[0m, in \u001B[0;36mcosine_similarity\u001B[1;34m(X, Y, dense_output)\u001B[0m\n\u001B[0;32m   1635\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001B[39;00m\n\u001B[0;32m   1636\u001B[0m \n\u001B[0;32m   1637\u001B[0m \u001B[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1675\u001B[0m \u001B[38;5;124;03m       [0.57..., 0.81...]])\u001B[39;00m\n\u001B[0;32m   1676\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;66;03m# to avoid recursive import\u001B[39;00m\n\u001B[1;32m-> 1679\u001B[0m X, Y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_pairwise_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1681\u001B[0m X_normalized \u001B[38;5;241m=\u001B[39m normalize(X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m Y:\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:185\u001B[0m, in \u001B[0;36mcheck_pairwise_arrays\u001B[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001B[0m\n\u001B[0;32m    175\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m    176\u001B[0m         X,\n\u001B[0;32m    177\u001B[0m         accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    182\u001B[0m         ensure_2d\u001B[38;5;241m=\u001B[39mensure_2d,\n\u001B[0;32m    183\u001B[0m     )\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 185\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    194\u001B[0m     Y \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m    195\u001B[0m         Y,\n\u001B[0;32m    196\u001B[0m         accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    201\u001B[0m         ensure_2d\u001B[38;5;241m=\u001B[39mensure_2d,\n\u001B[0;32m    202\u001B[0m     )\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m precomputed:\n",
      "File \u001B[1;32m~\\Work\\Bachelor\\Bachelor\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1058\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m   1053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1054\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1055\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1056\u001B[0m     )\n\u001B[0;32m   1057\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nd \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[1;32m-> 1058\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1059\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1060\u001B[0m         \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m   1061\u001B[0m     )\n\u001B[0;32m   1063\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[0;32m   1064\u001B[0m     _assert_all_finite(\n\u001B[0;32m   1065\u001B[0m         array,\n\u001B[0;32m   1066\u001B[0m         input_name\u001B[38;5;241m=\u001B[39minput_name,\n\u001B[0;32m   1067\u001B[0m         estimator_name\u001B[38;5;241m=\u001B[39mestimator_name,\n\u001B[0;32m   1068\u001B[0m         allow_nan\u001B[38;5;241m=\u001B[39mforce_all_finite \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1069\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c87e0e86d6a3bd05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
